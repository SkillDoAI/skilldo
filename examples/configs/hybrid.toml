# Hybrid â€” cheap local model for extraction, strong cloud model for validation
# Best of both worlds: fast + accurate
[llm]
provider = "openai-compatible"
model = "qwen3-coder:latest"
api_key_env = "none"
base_url = "http://localhost:11434/v1"
max_tokens = 16384

[generation]
max_retries = 5
max_source_tokens = 100000
enable_agent5 = true
agent5_mode = "thorough"

# Agent 5 uses GPT-5.2 for code generation (better at writing test code)
[generation.agent5_llm]
provider = "openai"
model = "gpt-5.2"
api_key_env = "OPENAI_API_KEY"

[generation.container]
runtime = "docker"
timeout = 1800
