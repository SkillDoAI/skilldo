# ╔═══════════════════════════════════════════════════════════════════╗
# ║  Skilldo — Full Reference Configuration                         ║
# ║                                                                  ║
# ║  Every field, every option. Copy what you need.                  ║
# ║  Values shown are illustrative — see Config::default() for      ║
# ║  actual runtime defaults.                                       ║
# ║  Most users only need [llm] + [generation]. The rest is tuning.  ║
# ╚═══════════════════════════════════════════════════════════════════╝
#
# Config search order:
#   1. --config <path>           (explicit CLI argument)
#   2. ./skilldo.toml            (repository root)
#   3. ~/.config/skilldo/config.toml  (user config directory)


# ── Main LLM ─────────────────────────────────────────────────────────
# Used by Agents 1-4 by default. Agent 5 uses this too unless
# [generation.agent5_llm] is set.
#
# Per-agent overrides: [generation.agent1_llm] through [generation.agent4_llm]
# let you run individual agents on different models. See below.

[llm]
# Provider: "anthropic", "openai", "gemini", or "openai-compatible"
provider = "anthropic"

# Model name (provider-specific)
model = "claude-sonnet-4-5-20250929"

# Environment variable containing the API key.
# Skilldo reads the key from this env var at runtime — never stores keys in config.
# Set to "none" for local models (Ollama) that don't need a key.
api_key_env = "ANTHROPIC_API_KEY"

# Base URL — only needed for openai-compatible providers (Ollama, DeepSeek, etc.)
# base_url = "http://localhost:11434/v1"

# Override max output tokens per LLM request.
# If not set, uses provider-specific defaults:
#   anthropic: 8192
#   openai: 8192
#   openai-compatible: 16384  (local models benefit from larger buffers)
#   gemini: 8192
# max_tokens = 8192

# Max retries for transient network errors (connection drops, 429s, 5xx).
# Set to 0 to disable retries entirely.
# network_retries = 10

# Delay between network retries in seconds (constant interval).
# With default 10 retries × 120s = 20 minutes max wait for rate limits.
# retry_delay = 120

# Extra fields merged into the LLM request body.
# Use for provider-specific parameters not covered by standard fields.
# These are merged at the top level of the JSON request payload.
#
# Two ways to specify extra fields (use whichever is cleaner for your case):
#
# Option 1: TOML tables (good for a few fields)
# [llm.extra_body]
# truncate = "END"
# top_p = 0.9
#
# [llm.extra_body.reasoning]
# effort = "high"
#
# Option 2: Raw JSON string (good for complex payloads, easy to copy from API docs)
# extra_body_json = '{"reasoning": {"effort": "high"}, "truncate": "END", "top_p": 0.9}'
#
# If both are set, they are merged. On key conflicts, extra_body_json wins.


# ── Generation Settings ──────────────────────────────────────────────

[generation]
# Max retry attempts for the generate → validate → fix loop.
# Each retry re-runs Agent 4 with error feedback.
# Increase for local models (10+) — they sometimes need several passes.
max_retries = 5

# Run agents 1-3 in parallel (default: true).
# Disable for local models (Ollama) to avoid overloading the machine.
# CLI: --no-parallel
parallel_extraction = true

# Approximate token budget for source code sent to agents.
# Larger libraries may need more; small libraries waste tokens at high values.
max_source_tokens = 100000

# Enable Agent 5 code validation in containers.
# When true, generated test code runs in Docker/Podman to verify patterns work.
# Disable with --no-agent5 on CLI or set to false here.
enable_agent5 = true

# Agent 5 validation mode:
#   "thorough"  — test every extracted pattern (default, most accurate)
#   "adaptive"  — test patterns, reduce scope on repeated failures
#   "minimal"   — test only core import + one pattern (fastest)
agent5_mode = "thorough"


# ── Agent 5 LLM Override ─────────────────────────────────────────────
# Use a different model for Agent 5 code generation.
# This is the "hybrid" setup: cheap model for extraction (agents 1-4),
# strong model for writing test code (agent 5).
#
# If not set, Agent 5 uses the main [llm] config.

# [generation.agent5_llm]
# provider = "openai"
# model = "gpt-5.2"
# api_key_env = "OPENAI_API_KEY"
# base_url = ""
# max_tokens = 8192
# network_retries = 10
# retry_delay = 120


# ── Per-Agent LLM Overrides (Advanced) ───────────────────────────────
# Run individual agents on different providers/models.
# Each is optional — if not set, the agent uses [llm].
#
# Use case: run cheap local extraction (agents 1-3) with a strong
# cloud model for synthesis (agent 4) and validation (agent 5).
#
# Each section has the same fields as [llm].

# [generation.agent1_llm]
# provider = "openai-compatible"
# model = "qwen3-coder:latest"
# api_key_env = "none"
# base_url = "http://localhost:11434/v1"

# [generation.agent2_llm]
# provider = "openai-compatible"
# model = "qwen3-coder:latest"
# api_key_env = "none"
# base_url = "http://localhost:11434/v1"

# [generation.agent3_llm]
# provider = "openai-compatible"
# model = "qwen3-coder:latest"
# api_key_env = "none"
# base_url = "http://localhost:11434/v1"

# [generation.agent4_llm]
# provider = "anthropic"
# model = "claude-sonnet-4-5-20250929"
# api_key_env = "ANTHROPIC_API_KEY"


# ── Container Settings ───────────────────────────────────────────────
# Agent 5 runs generated test code inside containers for safety.

[generation.container]
# Container runtime: "docker" or "podman"
runtime = "docker"

# Timeout for container execution in seconds.
# Increase for libraries with heavy C dependencies (numpy, scipy, pytorch)
# — pip install takes time.
timeout = 60

# Auto-remove containers after execution.
cleanup = true

# Library install source for Agent 5 validation:
#   "registry"       — install from PyPI (default, works for published packages)
#   "local-install"  — mount local repo at /src, run `pip install /src`
#                      (requires pyproject.toml or setup.py in the repo)
#   "local-mount"    — mount local repo, add to PYTHONPATH
#                      (no packaging needed, works with any directory structure)
# install_source = "registry"

# Path to local source repo for local-install or local-mount modes.
# Only used when install_source is "local-install" or "local-mount".
# Defaults to the PATH argument passed to `skilldo generate`.
# source_path = "/path/to/local/repo"

# Extra environment variables passed into containers.
# Use for private registries, proxies, or any ecosystem-specific config.
# [generation.container.extra_env]
# UV_EXTRA_INDEX_URL = "https://pypi.corp.com/simple/"
# HTTP_PROXY = "http://proxy:8080"
# HTTPS_PROXY = "http://proxy:8080"
# NO_PROXY = "localhost,127.0.0.1"
# NPM_CONFIG_REGISTRY = "https://npm.corp.com/"
# GOPROXY = "https://goproxy.corp.com,direct"
# GOPRIVATE = "corp.com/*"

# Container images — override if you need specific versions.
# python_image = "ghcr.io/astral-sh/uv:python3.11-bookworm-slim"
# javascript_image = "node:20-slim"
# rust_image = "rust:1.75-slim"
# go_image = "golang:1.21-alpine"


# ── Custom Prompts (Advanced) ────────────────────────────────────────
# Override or extend the built-in agent prompts.
#
# Per-agent mode:
#   "append"    — your text is added after the built-in prompt (default)
#   "overwrite" — your text replaces the built-in prompt entirely
#
# Agent 5 is always append-only (overwrite is ignored for safety).
#
# The global override_prompts flag sets the default for all agents.
# Per-agent modes take precedence over the global flag.

[prompts]
# Global default: false = append, true = overwrite
# override_prompts = false

# Per-agent mode overrides
# agent1_mode = "append"
# agent2_mode = "append"
# agent3_mode = "append"
# agent4_mode = "append"

# Custom prompt text per agent
# agent1_custom = "Also extract all class methods that start with 'get_'"
# agent2_custom = "Focus on async/await patterns"
# agent3_custom = "Pay special attention to migration guides"
# agent4_custom = "Use British English in all descriptions"
# agent5_custom = "Test edge cases with empty inputs"
