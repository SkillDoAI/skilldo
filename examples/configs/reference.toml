# ╔═══════════════════════════════════════════════════════════════════╗
# ║  Skilldo — Full Reference Configuration                         ║
# ║                                                                  ║
# ║  Every field, every option. Copy what you need.                  ║
# ║  Values shown are illustrative — see Config::default() for      ║
# ║  actual runtime defaults.                                       ║
# ║  Most users only need [llm] + [generation]. The rest is tuning.  ║
# ╚═══════════════════════════════════════════════════════════════════╝
#
# Config search order:
#   1. --config <path>           (explicit CLI argument)
#   2. ./skilldo.toml            (repository root)
#   3. ~/.config/skilldo/config.toml  (user config directory)


# ── Main LLM ─────────────────────────────────────────────────────────
# Used by all stages by default. Test stage uses this too unless
# [generation.test_llm] is set.
#
# Per-stage overrides: [generation.extract_llm] through [generation.create_llm]
# let you run individual stages on different models. See below.

[llm]
# Provider: "anthropic", "openai", "gemini", or "openai-compatible"
provider = "anthropic"

# Model name (provider-specific)
model = "claude-sonnet-4-5-20250929"

# Environment variable containing the API key.
# Skilldo reads the key from this env var at runtime — never stores keys in config.
# Set to "none" for local models (Ollama) that don't need a key.
api_key_env = "ANTHROPIC_API_KEY"

# Base URL — only needed for openai-compatible providers (Ollama, DeepSeek, etc.)
# base_url = "http://localhost:11434/v1"

# Override max output tokens per LLM request.
# If not set, uses provider-specific defaults:
#   anthropic: 8192
#   openai: 8192
#   openai-compatible: 16384  (local models benefit from larger buffers)
#   gemini: 8192
# max_tokens = 8192

# Max retries for transient network errors (connection drops, 429s, 5xx).
# Set to 0 to disable retries entirely.
# network_retries = 10

# Delay between network retries in seconds (constant interval).
# With default 10 retries × 120s = 20 minutes max wait for rate limits.
# retry_delay = 120

# Extra fields merged into the LLM request body.
# Use for provider-specific parameters not covered by standard fields.
# These are merged at the top level of the JSON request payload.
#
# Two ways to specify extra fields (use whichever is cleaner for your case):
#
# Option 1: TOML tables (good for a few fields)
# [llm.extra_body]
# truncate = "END"
# top_p = 0.9
#
# [llm.extra_body.reasoning]
# effort = "high"
#
# Option 2: Raw JSON string (good for complex payloads, easy to copy from API docs)
# extra_body_json = '{"reasoning": {"effort": "high"}, "truncate": "END", "top_p": 0.9}'
#
# If both are set, they are merged. On key conflicts, extra_body_json wins.


# ── Generation Settings ──────────────────────────────────────────────

[generation]
# Max retry attempts for the generate → validate → fix loop.
# Each retry re-runs the create stage with error feedback.
# Increase for local models (10+) — they sometimes need several passes.
max_retries = 5

# Run agents 1-3 in parallel (default: true).
# Disable for local models (Ollama) to avoid overloading the machine.
# CLI: --no-parallel
parallel_extraction = true

# Approximate token budget for source code sent to agents.
# Larger libraries may need more; small libraries waste tokens at high values.
max_source_tokens = 100000

# Enable test stage code validation in containers.
# When true, generated test code runs in Docker/Podman to verify patterns work.
# Disable with --no-test on CLI or set to false here.
enable_test = true

# Test stage validation mode:
#   "thorough"  — test every extracted pattern (default, most accurate)
#   "adaptive"  — test patterns, reduce scope on repeated failures
#   "minimal"   — test only core import + one pattern (fastest)
test_mode = "thorough"

# Enable review agent accuracy/safety validation (default: true).
# Reviews the generated SKILL.md for factual accuracy (API signatures,
# dates, versions) using container introspection, plus safety checks
# (prompt injection, obfuscated code).
# Disable with --no-review on CLI or set to false here.
enable_review = true

# Max retries for review → create feedback loop (default: 5).
# If review finds issues, it sends targeted complaints to the create agent.
# review_max_retries = 5


# ── Review Stage LLM Override ─────────────────────────────────────────
# Use a different model for the review stage.
# If not set, the review stage uses the main [llm] config.

# [generation.review_llm]
# provider = "anthropic"
# model = "claude-sonnet-4-5-20250929"
# api_key_env = "ANTHROPIC_API_KEY"


# ── Test Stage LLM Override ──────────────────────────────────────────
# Use a different model for the test stage code generation.
# This is the "hybrid" setup: cheap model for extraction (extract/map/learn),
# strong model for writing test code (test stage).
#
# If not set, the test stage uses the main [llm] config.

# [generation.test_llm]
# provider = "openai"
# model = "gpt-5.2"
# api_key_env = "OPENAI_API_KEY"
# base_url = ""
# max_tokens = 8192
# network_retries = 10
# retry_delay = 120


# ── Per-Stage LLM Overrides (Advanced) ───────────────────────────────
# Run individual stages on different providers/models.
# Each is optional — if not set, the stage uses [llm].
#
# Use case: run cheap local extraction (extract/map/learn) with a strong
# cloud model for synthesis (create) and validation (test).
#
# Each section has the same fields as [llm].

# [generation.extract_llm]
# provider = "openai-compatible"
# model = "qwen3-coder:latest"
# api_key_env = "none"
# base_url = "http://localhost:11434/v1"

# [generation.map_llm]
# provider = "openai-compatible"
# model = "qwen3-coder:latest"
# api_key_env = "none"
# base_url = "http://localhost:11434/v1"

# [generation.learn_llm]
# provider = "openai-compatible"
# model = "qwen3-coder:latest"
# api_key_env = "none"
# base_url = "http://localhost:11434/v1"

# [generation.create_llm]
# provider = "anthropic"
# model = "claude-sonnet-4-5-20250929"
# api_key_env = "ANTHROPIC_API_KEY"


# ── Container Settings ───────────────────────────────────────────────
# The test stage runs generated test code inside containers for safety.

[generation.container]
# Container runtime: "podman" or "docker" (auto-detected if omitted)
runtime = "docker"

# Timeout for container execution in seconds.
# Increase for libraries with heavy C dependencies (numpy, scipy, pytorch)
# — pip install takes time.
timeout = 60

# Auto-remove containers after execution.
cleanup = true

# Library install source for test stage validation:
#   "registry"       — install from PyPI (default, works for published packages)
#   "local-install"  — mount local repo at /src, run `pip install /src`
#                      (requires pyproject.toml or setup.py in the repo)
#   "local-mount"    — mount local repo, add to PYTHONPATH
#                      (no packaging needed, works with any directory structure)
# install_source = "registry"

# Path to local source repo for local-install or local-mount modes.
# Only used when install_source is "local-install" or "local-mount".
# Defaults to the PATH argument passed to `skilldo generate`.
# source_path = "/path/to/local/repo"

# Extra environment variables passed into containers.
# Use for private registries, proxies, or any ecosystem-specific config.
# [generation.container.extra_env]
# UV_EXTRA_INDEX_URL = "https://pypi.corp.com/simple/"
# HTTP_PROXY = "http://proxy:8080"
# HTTPS_PROXY = "http://proxy:8080"
# NO_PROXY = "localhost,127.0.0.1"
# NPM_CONFIG_REGISTRY = "https://npm.corp.com/"
# GOPROXY = "https://goproxy.corp.com,direct"
# GOPRIVATE = "corp.com/*"

# Container images — override if you need specific versions.
# python_image = "ghcr.io/astral-sh/uv:python3.11-bookworm-slim"
# javascript_image = "node:20-slim"
# rust_image = "rust:1.75-slim"
# go_image = "golang:1.21-alpine"


# ── Custom Prompts (Advanced) ────────────────────────────────────────
# Override or extend the built-in stage prompts.
#
# Per-stage mode:
#   "append"    — your text is added after the built-in prompt (default)
#   "overwrite" — your text replaces the built-in prompt entirely
#
# Test and review stages are always append-only (overwrite is ignored for safety).
#
# The global override_prompts flag sets the default for all stages.
# Per-stage modes take precedence over the global flag.

[prompts]
# Global default: false = append, true = overwrite
# override_prompts = false

# Per-stage mode overrides
# extract_mode = "append"
# map_mode = "append"
# learn_mode = "append"
# create_mode = "append"

# Custom prompt text per stage
# extract_custom = "Also extract all class methods that start with 'get_'"
# map_custom = "Focus on async/await patterns"
# learn_custom = "Pay special attention to migration guides"
# create_custom = "Use British English in all descriptions"
# review_custom = "Also check that all URLs in examples are reachable"
# test_custom = "Test edge cases with empty inputs"
